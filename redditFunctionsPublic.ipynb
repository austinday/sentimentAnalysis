{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import praw\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSubredditID(url):\n",
    "    m = re.search('www.reddit.com/r/(.+)/', url)\n",
    "    if m: \n",
    "        return m.group(1)\n",
    "    else: \n",
    "        return 'Unable to identify subreddit ID.'\n",
    "\n",
    "#Give it a url that is from reddit\n",
    "#Return all the titles (main page) or comments (comments page) along with metadata (user names, suburls, post time, etc...)\n",
    "def scrapeRedditPage(url, threadDepth=9999, commentDepth=9999):\n",
    "    pageInformation = {}\n",
    "    \n",
    "    #Log onto reddit using praw\n",
    "    reddit = loginToReddit()\n",
    "\n",
    "    threadInformation = []\n",
    "    for submission in reddit.subreddit(getSubredditID(url)).hot(limit=threadDepth):\n",
    "        try: \n",
    "            submissionInformation = {}\n",
    "            submissionInformation['title'] = submission.title #Title of the thread\n",
    "            submissionInformation['url'] = submission.url #Url that the tread points to, can be to outside or inside redit\n",
    "            submissionInformation['timeStamp'] = submission.created_utc #Time the thread was created\n",
    "            submissionInformation['author'] = submission.author #The author of the thread\n",
    "            submissionInformation['ups'] = submission.ups #The number of \"ups\" for the thread\n",
    "            submissionInformation['likes'] = submission.likes #The number of \"likes\" for the thread\n",
    "            submissionInformation['downs'] = submission.downs #The number of \"downs\" for the thread\n",
    "            submissionInformation['id'] = submission.id #The ID used to access the comments page\n",
    "            submissionInformation['numberComments'] = submission.num_comments #Number of comments in the thread\n",
    "            submissionInformation['score'] = submission.score #The reddit score of the thread\n",
    "            submissionInformation['text'] = submission.selftext #The text in the body of the thread post\n",
    "            submissionInformation['bestComments'] = [] #The top X comments ordered by the best to worst by score\n",
    "            submissionInformation['newestComments'] = [] #The top X comments ordered by the newest to oldest\n",
    "\n",
    "            commentSubmission = reddit.submission(id=submissionInformation['id'])\n",
    "            commentSubmission.comment_sort = 'best'\n",
    "            commentCount = 0 \n",
    "            for top_level_comment in commentSubmission.comments:\n",
    "                commentCount += 1\n",
    "                if commentCount > commentDepth:\n",
    "                    break\n",
    "                submissionInformation['bestComments'].append((top_level_comment.body, top_level_comment.score, top_level_comment.created_utc, top_level_comment.author))\n",
    "\n",
    "            commentSubmission.comment_sort = 'new'\n",
    "            commentCount = 0 \n",
    "            for top_level_comment in commentSubmission.comments:\n",
    "                commentCount += 1\n",
    "                if commentCount > commentDepth:\n",
    "                    break\n",
    "                submissionInformation['newestComments'].append((top_level_comment.body, top_level_comment.score, top_level_comment.created_utc, top_level_comment.author))\n",
    "            threadInformation.append(submissionInformation)\n",
    "        except: \n",
    "            print(\"Error, skipped a submission thread.\")\n",
    "            pass\n",
    "    return threadInformation\n",
    "\n",
    "\n",
    "def getAllTextFromRedditPage(redditName, maxThreads=999999, maxComments=99999):\n",
    "    #All google main \"cryonics\" search page result pages\n",
    "\n",
    "    identifierString = redditName + \"_\" + str(maxThreads) + \"_\" + str(maxComments)\n",
    "\n",
    "    try:\n",
    "        print(\"Attempting to load redditSearchAllText\"+identifierString+\".p variable from previous calculation.\")\n",
    "        redditAllText = pickle.load(open(\"redditSearchAllText\"+identifierString+\".p\", \"rb\"))\n",
    "        allUrls = pickle.load(open(\"redditSearchUrls\"+identifierString+\".p\", \"rb\"))\n",
    "        sentenceInformation = pickle.load(open(\"sentenceInformation\"+identifierString+\".p\", \"rb\"))\n",
    "        print(\"Load successful.\")    \n",
    "    except (OSError, IOError) as e:\n",
    "        print(\"Load un-successful. Calculating.\")\n",
    "        \n",
    "        redditUrls = []\n",
    "        redditAllText = \"\"\n",
    "        sentenceInformation = {}\n",
    "        print(\"Getting reddit threads information.\")\n",
    "        threadInformation = scrapeRedditPage(\"https://www.reddit.com/r/\"+redditName+\"/\", threadDepth=maxThreads, commentDepth=maxComments)\n",
    "        print(\"Finished getting reddit threads information.\")\n",
    "        for x in threadInformation: \n",
    "            \n",
    "            #Add Thread information\n",
    "            url = x[\"url\"]\n",
    "            threadAuthor = x[\"author\"]\n",
    "            redditUrls.append(x[\"url\"])\n",
    "            redditAllText = redditAllText + x[\"title\"]\n",
    "            sentenceInformation[x[\"title\"]] = (url, threadAuthor, getSentencesSentiment([x[\"title\"]]))\n",
    "            print(\"Found thread at: \", url, \" by author: \", threadAuthor)\n",
    "            commentObject = x[\"bestComments\"]\n",
    "            #Add all comments\n",
    "            for commentCount in range(0, len(commentObject)): \n",
    "                commentText = x[\"bestComments\"][commentCount][0]\n",
    "                redditAllText = redditAllText + x[\"bestComments\"][commentCount][0]\n",
    "\n",
    "                commentScore = x[\"bestComments\"][commentCount][1]\n",
    "                commentCreatedTimestamp = x[\"bestComments\"][commentCount][2]\n",
    "                commentAuthor = x[\"bestComments\"][commentCount][3]\n",
    "\n",
    "                sentenceInformation[commentText] = (url, threadAuthor, getSentencesSentiment([commentText]), commentAuthor, x[\"title\"], )\n",
    "                print(\"Found comment at: \", url, \" by author: \", commentAuthor)\n",
    "            print(\"Size of sentence information: \", len(sentenceInformation))\n",
    "            \n",
    "\n",
    "        print(\"Writing calculated redditSearch\"+identifierString+\".p variable to pickle file.\")\n",
    "        \n",
    "        pickle.dump(redditAllText, open( \"redditSearchAllText\"+identifierString+\".p\", \"wb\" ) )\n",
    "        pickle.dump(redditUrls, open( \"redditSearchUrls\"+identifierString+\".p\", \"wb\" ) )\n",
    "        pickle.dump(sentenceInformation, open( \"sentenceInformation\"+identifierString+\".p\", \"wb\" ) )\n",
    "        \n",
    "    return (redditAllText, sentenceInformation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
