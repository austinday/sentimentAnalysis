{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import sqlite3\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import json\n",
    "import praw\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import collections\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "#To import jupyter notebooks\n",
    "import nbimporter\n",
    "\n",
    "from seleniumFunctions import *\n",
    "from seleniumRedditFunctions import *\n",
    "from redditFunctionsPublic import *\n",
    "from twitterFunctionsPublic import *\n",
    "from visualizationFunctions import *\n",
    "from functionsForSentimentAnalysis import *\n",
    "\n",
    "#Private notebooks with sensitive information. \n",
    "from redditFunctionsPrivate import *\n",
    "from twitterFunctionsPrivate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Google Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchTerms = [\"\\\"Donald Trump\\\"\", \"\\\"Trump\\\"\"]\n",
    "identifier = \"Google_Search\"\n",
    "for term in searchTerms: \n",
    "    identifier = identifier + \"_\" + str(term)\n",
    "searchDepth = 10\n",
    "maxDepth = 1\n",
    "identifier = identifier + \"Google_Page_Search_Depth_\" + str(searchDepth)\n",
    "              \n",
    "(allText, allSentencesAndLocations, basicSentenceSentiment) = getAllTextFromGoogleSearch(searchTerms, searchDepth, maxDepth)\n",
    "allAllText = \"\"\n",
    "for sentence in allText: \n",
    "    allAllText = allAllText + sentence\n",
    "allText = allAllText\n",
    "\n",
    "makeWordCloud(allText, \"DonaldTrumpWordCloud\")\n",
    "printBasicSentenceSentimentToCsv(basicSentenceSentiment, \"DonaldTrumpTabFile\")\n",
    "print(\"Finished creating word cloud and sentiment csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Twitter Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Attempting to load tweets from disk.\")\n",
    "    DonaldTrumpTweets = pickle.load(open(\"DonaldTrumpTweets.p\", \"rb\"))\n",
    "except: \n",
    "    print(\"Cannot load from disk. Gathering tweets.\")\n",
    "    DonaldTrumpTweets = getTweets(\"Donald Trump\")\n",
    "    output = open(\"DonaldTrumpTweets.p\", \"wb\")\n",
    "    pickle.dump(DonaldTrumpTweets, output)\n",
    "    output.close()\n",
    "    print(\"Done gathering tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Reddit Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    print(\"Attempting to load Reddit information from disk.\")\n",
    "    The_DonaldRedditAllText = pickle.load(open(\"The_DonaldRedditAllText.p\", \"rb\"))\n",
    "    The_DonaldRedditInformation = pickle.load(open(\"The_DonaldRedditInformation.p\", \"rb\"))\n",
    "except: \n",
    "    print(\"Gather Reddit information.\")\n",
    "    (The_DonaldRedditAllText, The_DonaldRedditInformation) = getAllTextFromRedditPage(\"The_Donald\", maxThreads=100000000, maxComments=100000000)\n",
    "    output1 = open(\"The_DonaldRedditAllText.p\", \"wb\")\n",
    "    output2 = open(\"The_DonaldRedditInformation.p\", \"wb\")\n",
    "    pickle.dump(The_DonaldRedditAllText, output1)\n",
    "    pickle.dump(The_DonaldRedditInformation, output2)\n",
    "    output1.close() \n",
    "    output2.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
