{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from redditFunctionsPrivate.ipynb\n",
      "Importing Jupyter notebook from twitterFunctionsPrivate.ipynb\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import sqlite3\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import json\n",
    "import praw\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import collections\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "#To import jupyter notebooks\n",
    "import nbimporter\n",
    "\n",
    "from seleniumFunctions import *\n",
    "from seleniumRedditFunctions import *\n",
    "from redditFunctionsPublic import *\n",
    "from twitterFunctionsPublic import *\n",
    "from visualizationFunctions import *\n",
    "\n",
    "#Private notebooks with sensitive information. \n",
    "from redditFunctionsPrivate import *\n",
    "from twitterFunctionsPrivate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordWeights(allText, searchTerms):\n",
    "    wordHash = {}\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    additionalStopWords = [\"-\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \".\", \"Sign in\", \"Feedback\"]\n",
    "    for x in additionalStopWords: \n",
    "        stop_words.add(x)\n",
    "\n",
    "    allTextOld = \"\"\n",
    "    for sentence in allText: \n",
    "        allTextOld = allTextOld + sentence\n",
    "\n",
    "    for word in stop_words: \n",
    "        compiledRegex = re.compile(\"\\s\"+word+\"\\s\", re.IGNORECASE)\n",
    "        allTextOld = compiledRegex.sub(\"\", allTextOld)\n",
    "\n",
    "    for word in allText.split(): \n",
    "        if word in wordHash: \n",
    "            wordHash[word] = wordHash[word] + 1\n",
    "        else: \n",
    "            wordHash[word] = 1\n",
    "\n",
    "    wordWeights = {}\n",
    "    s = [(k, wordHash[k]) for k in sorted(wordHash, key=wordHash.get, reverse=True)]\n",
    "    for k, v in s:\n",
    "        wordWeights[k] = int(v)\n",
    "        \n",
    "    minValue = 9999\n",
    "    maxValue = -9999\n",
    "    for x in wordWeights: \n",
    "        key = x\n",
    "        value = wordWeights[key]\n",
    "        if value > maxValue:\n",
    "            maxValue = value\n",
    "        if value < minValue: \n",
    "            minValue = value\n",
    "\n",
    "    for x in wordWeights: \n",
    "        key = x\n",
    "        value = float(wordWeights[key])\n",
    "        wordWeights[key] = (value-minValue) / (maxValue-minValue)\n",
    "\n",
    "    return wordWeights\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    caps = \"([A-Z])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    longSentences = []\n",
    "    for sentence in sentences: \n",
    "        if len(sentence) > 20:\n",
    "            longSentences.append(sentence)\n",
    "    return longSentences\n",
    "\n",
    "def getSentencesSentiment(sentenceArray):\n",
    "    allSentiments = {}\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for sentence in sentenceArray:\n",
    "        \n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        sentiments = {}\n",
    "        for k in ss:\n",
    "            sentiments[k] = ss[k]\n",
    "        allSentiments[sentence] = sentiments\n",
    "    return allSentiments\n",
    "\n",
    "def allTextToSentiment(allText, searchTerms, sortBy=\"compound\"):\n",
    "    sentences = allText\n",
    "    sentiment = getSentencesSentiment(sentences)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sortedSentiments = sorted(sentiment.keys(), key=lambda x: sentiment[x][\"pos\"], reverse=True)\n",
    "    allSentiment = []\n",
    "    for sentence in sortedSentiments:\n",
    "        allSentiment.append((sentence, sentiment[sentence]))\n",
    "        \n",
    "        \n",
    "    newSentiment = {}\n",
    "    wordWeights = getWordWeights(allText, searchTerms)\n",
    "    #count1 = 0\n",
    "    for x in allSentiment: \n",
    "        #print(\"count: \", count1, \" of: \", len(allSentiment))\n",
    "        #count1 += 1\n",
    "        sentence = x[0]\n",
    "        wordList = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
    "        \n",
    "        if len(wordList) < 5: \n",
    "            pass\n",
    "        else: \n",
    "            sentimentHash = x[1] #Hash table\n",
    "            importance = 0.0\n",
    "            usedWeights = []\n",
    "            count = 0\n",
    "\n",
    "            for word in wordList: \n",
    "                count += 1\n",
    "                try: \n",
    "                    usedWeights.append(wordWeights[word])\n",
    "                except: \n",
    "                    pass\n",
    "\n",
    "            for weight in usedWeights: \n",
    "                importance = float(importance) + float(weight)\n",
    "\n",
    "            if len(usedWeights) > 0: \n",
    "                importance = float(importance) / float(count)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            sentimentHash[\"frequency\"] = importance    \n",
    "            overallCompound = (sentimentHash[\"frequency\"] * 30) + (sentimentHash[\"pos\"] * 10) + (sentimentHash[\"neg\"] * 10) + (sentimentHash[\"compound\"] * 7)\n",
    "            sentimentHash[\"freqPos\"] = sentimentHash[\"frequency\"]*1.5 + sentimentHash[\"pos\"] - 0.5 * sentimentHash[\"neu\"]\n",
    "            sentimentHash[\"freqNeg\"] = sentimentHash[\"frequency\"]*1.5 + sentimentHash[\"neg\"] - 0.5 * sentimentHash[\"neu\"]\n",
    "            sentimentHash[\"weightedCompound\"] = overallCompound\n",
    "\n",
    "            if sentimentHash[\"frequency\"] < 0.10: \n",
    "                sentimentHash[\"freqPos\"] = 0\n",
    "            \n",
    "            newSentiment[sentence] = sentimentHash\n",
    "            \n",
    "     \n",
    "    newSortedSentiments = sorted(newSentiment.keys(), key=lambda x: newSentiment[x][sortBy], reverse=True)\n",
    "    \n",
    "            \n",
    "    allSentimentFinal = []\n",
    "    for sentence in newSortedSentiments:\n",
    "        allSentimentFinal.append((sentence, newSentiment[sentence]))\n",
    "            \n",
    "    return allSentimentFinal\n",
    "\n",
    "\n",
    "\n",
    "def getGoogleSearchResults(searchTerm, numPages):\n",
    "    from selenium import webdriver\n",
    "    driver = webdriver.Firefox()\n",
    "\n",
    "    driver.get(\"https://www.google.com/search?q=\"+str(searchTerm))\n",
    "    links = []\n",
    "    \n",
    "    results = driver.find_elements_by_css_selector('div.g')\n",
    "    for result in results: \n",
    "        link = result.find_element_by_tag_name(\"a\")\n",
    "        href = link.get_attribute(\"href\")\n",
    "        links.append(href)\n",
    "    links.append(driver.current_url)\n",
    "    \n",
    "    \n",
    "    for pageNum in range(1, numPages):\n",
    "        driver.find_element_by_link_text(\"Next\").click()\n",
    "        results = driver.find_elements_by_css_selector('div.g')\n",
    "        for result in results: \n",
    "            link = result.find_element_by_tag_name(\"a\")\n",
    "            href = link.get_attribute(\"href\")\n",
    "            links.append(href)\n",
    "        links.append(driver.current_url)\n",
    "    \n",
    "    #driver.close()\n",
    "    return links\n",
    "\n",
    "def getSentiment(allText, searchTerms, allSentencesAndLocations, sortBy=\"frequency\"):\n",
    "    sentiment = allTextToSentiment(allText, searchTerms, sortBy=sortBy) #Returns a list of tuples, each tuple is sentence, then hash of sentiment         \n",
    "    print(\"sentiment: \", sentiment)\n",
    "    overallSentiment = 0\n",
    "    print(\"Sorted by: \", sortBy)\n",
    "    allSentimentHash = {}\n",
    "    for example in sentiment: \n",
    "        sentence = example[0] + \"\\n\"\n",
    "        #print(\"sentence: \", sentence)\n",
    "        sentimentHash = example[1]\n",
    "        positive = round(sentimentHash[\"pos\"],2)\n",
    "        negative = round(sentimentHash[\"neg\"],2)\n",
    "        neutral = round(sentimentHash[\"neu\"],2)\n",
    "        intensity = round(sentimentHash[\"compound\"],2)\n",
    "        frequency = round(sentimentHash[\"frequency\"],2)\n",
    "        compound = round(sentimentHash[\"weightedCompound\"],2)\n",
    "        url = allSentencesAndLocations[sentence.replace(\"\\n\", \"\")]\n",
    "        sentimentHash[\"location\"] = url\n",
    "        overallSentiment = overallSentiment + positive  - negative\n",
    "        allSentimentHash[sentence] = sentimentHash\n",
    "        #print(\"\\\"\", sentence.strip(), \"\\\" \\nPositive: \", positive, \" Negative: \", negative, \" Neutral: \", neutral, \" Frequency: \", frequency, \" Intensity: \", intensity, \" Weighted Compound: \", compound)\n",
    "        #print(\"\")\n",
    "    if not float(len(sentiment)) == 0: \n",
    "        overallSentiment = 100 * (float(overallSentiment) / float(len(sentiment)))\n",
    "    #print(\"Overall Sentiment: \", overallSentiment)\n",
    "    return (overallSentiment, allSentimentHash)\n",
    "\n",
    "def getAllTextFromGoogleSearch(searchTerms, searchDepth, maxDepth):\n",
    "    #All google main \"cryonics\" search page result pages\n",
    "    allallText = \"\"\n",
    "    allSentencesAndLocations = {}\n",
    "\n",
    "    identifierString = \"\"\n",
    "    for searchTerm in searchTerms: \n",
    "        identifierString = identifierString + str(searchTerm)\n",
    "    identifierString = identifierString + str(maxDepth) + str(searchDepth)\n",
    "    identifierString = identifierString.replace(\" \", \"\")\n",
    "    identifierString = identifierString.replace(\"?\", \"\")\n",
    "    identifierString = identifierString.replace(\"'\", \"\")\n",
    "\n",
    "    try:\n",
    "        print(\"Attempting to load googleSearch\"+identifierString+\".p variable from previous calculation.\")\n",
    "        allallText = pickle.load(open(\"googleSearch\"+identifierString+\".p\", \"rb\"))\n",
    "        fixedSentenceUrlHash = pickle.load(open(\"googleSearchallSentencesAndLocations\"+identifierString+\".p\", \"rb\"))\n",
    "        basicSentimentList = pickle.load(open(\"basicSentimentList\"+identifierString+\".p\", \"rb\"))\n",
    "        print(\"Load successful.\")    \n",
    "    except (OSError, IOError) as e:\n",
    "        print(\"Load un-successful. Calculating.\")\n",
    "\n",
    "        links = []\n",
    "        for searchTerm in searchTerms: \n",
    "            tempLinks = getGoogleSearchResults(searchTerm, searchDepth)\n",
    "            links.extend(tempLinks)\n",
    "\n",
    "        links = list(set(links))\n",
    "        \n",
    "        allLinksList = links\n",
    "        allText = \"\"\n",
    "        \n",
    "        count = 0\n",
    "        driver = createSeleniumWebdriver(\"http://www.google.com\")\n",
    "        for depthValue in range(0, maxDepth): \n",
    "            newLinks = []\n",
    "            for link in allLinksList:\n",
    "                print(\"Processing link: \", count, \" out of: \", len(allLinksList))\n",
    "                count += 1\n",
    "                try: \n",
    "                    (text, urls) = getWebpageTextAndLinksSelenium(link, driver)\n",
    "                    allText = allText + text\n",
    "                    for url in urls: \n",
    "                        newLinks.extend(urls)\n",
    "                except: \n",
    "                    print(\"Skipping link: \", link)\n",
    "                    pass\n",
    "                \n",
    "                #If the link is already in the hash, add text to it, otherwise, make it and add the text\n",
    "                if link in allSentencesAndLocations: \n",
    "                    tempTextList = allSentencesAndLocations[link]\n",
    "                    tempTextList.append(allText)\n",
    "                    allSentencesAndLocations[link] = tempTextList\n",
    "                else:\n",
    "                    allSentencesAndLocations[link] = [allText]\n",
    "                allallText = allallText + allText\n",
    "                #print(\"Adding: \", allText)\n",
    "                allText = \"\"\n",
    "            \n",
    "            allLinksList = list(set(newLinks))\n",
    "\n",
    "        print(\"Writing calculated googleSearch\"+identifierString+\".p variable to pickle file.\")\n",
    "        output = open(\"googleSearch\"+identifierString+\".p\", \"wb\")\n",
    "        pickle.dump(allallText, output)\n",
    "        output.close()\n",
    "        \n",
    "        fixedSentenceUrlHash = {}\n",
    "        basicSentimentList = []\n",
    "        allallText = []\n",
    "        #Parse up the allSentencesAndLocations variable into key=sentence value = url\n",
    "        for url, allTextUrl in allSentencesAndLocations.items():\n",
    "            for allTextGroup in allTextUrl:\n",
    "                sentences = split_into_sentences(allTextGroup)\n",
    "                for sentence in sentences: \n",
    "                    fixedSentenceUrlHash[sentence] = url\n",
    "                    allallText.append(sentence)\n",
    "                    basicSentimentList.append((getSentencesSentiment([sentence]), url))\n",
    "                    \n",
    "        output = open(\"googleSearchallSentencesAndLocations\"+identifierString+\".p\", \"wb\")\n",
    "        pickle.dump(fixedSentenceUrlHash, output)\n",
    "        output.close()\n",
    "        \n",
    "        output = open(\"basicSentimentList\"+identifierString+\".p\", \"wb\")\n",
    "        pickle.dump(basicSentimentList, output)\n",
    "        output.close()\n",
    "\n",
    "    return (allallText, fixedSentenceUrlHash, basicSentimentList)\n",
    "              \n",
    "def printBasicSentenceSentimentToCsv(basicSentenceSentiment, csvName):\n",
    "    file = open(csvName+\".csv\",\"w\") \n",
    "    for sentimentTuple in basicSentenceSentiment:\n",
    "        infoHash = sentimentTuple[0]\n",
    "        url = sentimentTuple[1]\n",
    "        for sentence, sentiment in infoHash.items():\n",
    "            toWrite = sentence + \"\\t\" + str(url) + \"\\t\" + str(sentiment[\"pos\"]) + \"\\t\" + str(sentiment[\"neg\"]) + \"\\t\" + str(sentiment[\"neu\"]) + \"\\t\" + str(sentiment[\"compound\"]) + \"\\n\"\n",
    "            file.write(toWrite)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
